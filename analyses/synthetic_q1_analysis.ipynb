{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import soft_label_learning.experiments.statistical_tests as stac\n",
    "from soft_label_learning.config import path_output\n",
    "from soft_label_learning.experiments.experiment_settings import (\n",
    "    methods_with_threshold,\n",
    "    methods_without_threshold,\n",
    "    non_ens_methods,\n",
    "    q1_settings,\n",
    ")\n",
    "from soft_label_learning.experiments.process_synthetic_data import (\n",
    "    get_q1_result_dict,\n",
    "    plot_heatmap,\n",
    "    q1_statistics_result_dict,\n",
    "    q1_statistics_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the experiment base parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO set datetime to the desired result folder\n",
    "settings_dict, result_path = q1_settings, \"date_hh_mm_ss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising settings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_settings = {}\n",
    "\n",
    "# Code to take the first option\n",
    "for key in settings_dict.keys():\n",
    "    if key == \"dataset\":\n",
    "        fixed_settings[key] = settings_dict[key][0][:-4]\n",
    "    elif key == \"eval_set\":\n",
    "        fixed_settings[key] = settings_dict[key][fixed_settings[\"label_eval\"]][0]\n",
    "    elif key == \"metric\":\n",
    "        fixed_settings[key] = settings_dict[key][fixed_settings[\"label_eval\"]][0]\n",
    "    elif key in [\"ens_propagation\", \"smoothing\"]:\n",
    "        fixed_settings[key] = settings_dict[key]\n",
    "    else:\n",
    "        fixed_settings[key] = settings_dict[key][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_settings[\"ens_propagation\"] = True\n",
    "fixed_settings[\"gt\"] = \"rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the analyses either with or without the threshold methods\n",
    "threshold = False\n",
    "\n",
    "if threshold:\n",
    "    methods = [\"PluralityBootstrapClf\"] + methods_with_threshold\n",
    "    base_row = 0\n",
    "else:\n",
    "    base_row = 4\n",
    "    methods = methods_without_threshold.copy()\n",
    "\n",
    "datasets = [x[:-4] for x in settings_dict[\"dataset\"]]\n",
    "classifiers = [x for x in settings_dict[\"clf\"]]\n",
    "metrics = [\"TVD\", \"hard_soft_AUC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_labels = methods.copy()\n",
    "method_labels = [s.replace(\"_\", \" - \") for s in method_labels]\n",
    "dataset_labels = [s.replace(\"_\", \" \") for s in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_1 = fixed_settings.copy()\n",
    "rf_1[\"gt\"] = \"rf\"\n",
    "rf_1[\"mtvd\"] = \"1\"\n",
    "rf_2 = fixed_settings.copy()\n",
    "rf_2[\"gt\"] = \"rf\"\n",
    "rf_2[\"mtvd\"] = \"2\"\n",
    "lr_1 = fixed_settings.copy()\n",
    "lr_1[\"gt\"] = \"lr\"\n",
    "lr_1[\"mtvd\"] = \"1\"\n",
    "lr_2 = fixed_settings.copy()\n",
    "lr_2[\"gt\"] = \"lr\"\n",
    "lr_2[\"mtvd\"] = \"2\"\n",
    "\n",
    "result_dict_rf_1 = get_q1_result_dict(\n",
    "    rf_1,\n",
    "    methods,\n",
    "    metrics,\n",
    "    classifiers,\n",
    "    datasets,\n",
    "    non_ens_methods,\n",
    "    result_path,\n",
    ")\n",
    "\n",
    "result_dict_rf_2 = get_q1_result_dict(\n",
    "    rf_2,\n",
    "    methods,\n",
    "    metrics,\n",
    "    classifiers,\n",
    "    datasets,\n",
    "    non_ens_methods,\n",
    "    result_path,\n",
    ")\n",
    "\n",
    "result_dict_lr_1 = get_q1_result_dict(\n",
    "    lr_1,\n",
    "    methods,\n",
    "    metrics,\n",
    "    classifiers,\n",
    "    datasets,\n",
    "    non_ens_methods,\n",
    "    result_path,\n",
    ")\n",
    "result_dict_lr_2 = get_q1_result_dict(\n",
    "    lr_2,\n",
    "    methods,\n",
    "    metrics,\n",
    "    classifiers,\n",
    "    datasets,\n",
    "    non_ens_methods,\n",
    "    result_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_dict = {}\n",
    "\n",
    "for metric in result_dict_rf_1.keys():\n",
    "    for clf in result_dict_rf_1[metric].keys():\n",
    "        results = [\n",
    "            result_dict_rf_1[metric][clf],\n",
    "            result_dict_rf_2[metric][clf],\n",
    "            result_dict_lr_1[metric][clf],\n",
    "            result_dict_lr_2[metric][clf],\n",
    "        ]\n",
    "\n",
    "        average_results = np.mean(results, axis=0)\n",
    "\n",
    "        fig = plot_heatmap(\n",
    "            average_results,\n",
    "            xlabels=dataset_labels,\n",
    "            ylabels=method_labels,\n",
    "            base_row=base_row,\n",
    "        )\n",
    "\n",
    "        heatmap_dict[(metric, clf)] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for key, fig in heatmap_dict.items():\n",
    "        if not threshold:\n",
    "            fig.savefig(\n",
    "                path_output / \"Q1\" / f\"q1_heatmap_{key[0]}_{key[1]}.png\",\n",
    "                dpi=300,\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "        if threshold:\n",
    "            fig.savefig(\n",
    "                path_output / \"Q1\" / f\"q1_heatmap_threshold_{key[0]}_{key[1]}.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                dpi=300,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap over all settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_settings = settings_dict.copy()\n",
    "specific_settings[\"ens_propagation\"] = True\n",
    "specific_settings[\"noise_type\"] = \"noiseless\"\n",
    "specific_settings[\"noise\"] = \"0\"\n",
    "specific_settings[\"alpha\"] = 1\n",
    "\n",
    "metrics = [\"TVD\", \"hard_soft_AUC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = q1_statistics_result_dict(\n",
    "    specific_settings,\n",
    "    settings_dict,\n",
    "    result_path,\n",
    "    methods,\n",
    "    metrics,\n",
    "    non_ens_methods,\n",
    "    classifiers,\n",
    "    datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_array, clf_metric = q1_statistics_table(\n",
    "    result_dict,\n",
    "    settings_dict,\n",
    "    methods,\n",
    "    metrics,\n",
    "    classifiers,\n",
    "    datasets,\n",
    "    threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_heatmap(\n",
    "    table_array,\n",
    "    xlabels=clf_metric,\n",
    "    ylabels=method_labels,\n",
    "    base_row=base_row,\n",
    "    add_mean=False,\n",
    "    figsize=(8, 8),\n",
    ")\n",
    "\n",
    "if True:\n",
    "    if threshold:\n",
    "        fig.savefig(\n",
    "            path_output / \"Q1\" / \"q1_statistics_threshold.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    else:\n",
    "        fig.savefig(\n",
    "            path_output / \"Q1\" / \"q1_statistics.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [\n",
    "    \"Friedman\",\n",
    "    \"Hard\",\n",
    "    \"Soft\",\n",
    "    \"HardEns\",\n",
    "    \"SoftEns\",\n",
    "    \"vs Hard\",\n",
    "    \"vs Soft\",\n",
    "    \"vs HardEns\",\n",
    "]\n",
    "representative_methods = [\n",
    "    \"PluralityClf\",\n",
    "    \"DuplicateWeightsClf\",\n",
    "    \"PluralityEnsClf\",\n",
    "    \"BootstrapDupWeightsClf\",\n",
    "]\n",
    "\n",
    "complete_stat_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "complete_stat_dict = {}\n",
    "\n",
    "# Best performing of each group: PluralityClf, DuplicateWeightsClf,\n",
    "# PluralityEnsClf, BootstrapDupWeightsClf\n",
    "method_key_dict = {\n",
    "    \"PluralityClf\": 1,\n",
    "    \"DuplicateWeightsClf\": 3,\n",
    "    \"PluralityEnsClf\": 6,\n",
    "    \"BootstrapDupWeightsClf\": 10,\n",
    "}\n",
    "\n",
    "complete_stat_dict = {}\n",
    "\n",
    "for metric in result_dict_rf_1.keys():\n",
    "    for clf in result_dict_rf_1[metric].keys():\n",
    "        results = [\n",
    "            result_dict_rf_1[metric][clf],\n",
    "            result_dict_rf_2[metric][clf],\n",
    "            result_dict_lr_1[metric][clf],\n",
    "            result_dict_lr_2[metric][clf],\n",
    "        ]\n",
    "\n",
    "        method_stat_dict = {}\n",
    "\n",
    "        for method in method_key_dict.keys():\n",
    "            method_stat_dict[method] = np.array(results)[\n",
    "                :, method_key_dict[method], :\n",
    "            ].reshape(4 * len(q1_settings[\"dataset\"]))\n",
    "\n",
    "        samples = np.array(\n",
    "            [method_stat_dict[method] for method in method_stat_dict.keys()]\n",
    "        )\n",
    "\n",
    "        friedman_result = stac.friedman_aligned_ranks_test(\n",
    "            samples[0], samples[1], samples[2], samples[3]\n",
    "        )\n",
    "        print(f\"Friedman p-value: {friedman_result[1]}\")\n",
    "\n",
    "        if metric == \"hard_soft_AUC\":\n",
    "            metric_adjusted = \"AUC\"\n",
    "        else:\n",
    "            metric_adjusted = r\"$\\overline{TVD}$\"\n",
    "\n",
    "        complete_stat_dict[(metric_adjusted, clf)] = [friedman_result[1]]\n",
    "        complete_stat_dict[(metric_adjusted, clf)] += friedman_result[2]\n",
    "\n",
    "        pivot_dict = {\n",
    "            \"PluralityClf\": friedman_result[3][0],\n",
    "            \"DuplicateWeightsClf\": friedman_result[3][1],\n",
    "            \"PluralityEnsClf\": friedman_result[3][2],\n",
    "            \"BootstrapDupWeightsClf\": friedman_result[3][3],\n",
    "        }\n",
    "\n",
    "        finner_result = stac.finner_test(pivot_dict, \"BootstrapDupWeightsClf\")\n",
    "\n",
    "        print(finner_result[2])\n",
    "        print(finner_result[3])\n",
    "\n",
    "        complete_stat_dict[(metric_adjusted, clf)].append(\n",
    "            finner_result[3][\n",
    "                finner_result[0].index(\"BootstrapDupWeightsClf vs PluralityClf\")\n",
    "            ]\n",
    "        )\n",
    "        complete_stat_dict[(metric_adjusted, clf)].append(\n",
    "            finner_result[3][\n",
    "                finner_result[0].index(\"BootstrapDupWeightsClf vs DuplicateWeightsClf\")\n",
    "            ]\n",
    "        )\n",
    "        complete_stat_dict[(metric_adjusted, clf)].append(\n",
    "            finner_result[3][\n",
    "                finner_result[0].index(\"BootstrapDupWeightsClf vs PluralityEnsClf\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "stat_frame = pd.DataFrame.from_dict(\n",
    "    complete_stat_dict, orient=\"index\", columns=df_columns\n",
    ")\n",
    "stat_frame.index = pd.MultiIndex.from_tuples(stat_frame.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = stat_frame.copy()\n",
    "new_columns = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        (\"$p$-value\", formatted_df.columns[0]),\n",
    "        (\"Rank\", formatted_df.columns[1]),\n",
    "        (\"Rank\", formatted_df.columns[2]),\n",
    "        (\"Rank\", formatted_df.columns[3]),\n",
    "        (\"Rank\", formatted_df.columns[4]),\n",
    "        (\"adjusted $p$-value: SoftEns\", formatted_df.columns[5]),\n",
    "        (\"adjusted $p$-value: SoftEns\", formatted_df.columns[6]),\n",
    "        (\"adjusted $p$-value: SoftEns\", formatted_df.columns[7]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_df.columns = new_columns\n",
    "\n",
    "latex_df = formatted_df.style.format(\n",
    "    {\n",
    "        new_columns[0]: \"{:.2e}\",\n",
    "        new_columns[1]: \"{:.2f}\",\n",
    "        new_columns[2]: \"{:.2f}\",\n",
    "        new_columns[3]: \"{:.2f}\",\n",
    "        new_columns[4]: \"{:.2f}\",\n",
    "        new_columns[5]: \"{:.2e}\",\n",
    "        new_columns[6]: \"{:.2e}\",\n",
    "        new_columns[7]: \"{:.2e}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_df.to_latex().replace(\"0.00e+00\", \"0e0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soft_label_calibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
